# Feb 8 2020, sunny
# on GP
# https://distill.pub/2019/visual-exploration-gaussian-processes/
# reproduce figure like figure 2.2 in GPML textbook page 15


library("MASS")

# generate covariance matrix for points in `x` using given kernel function
cov.matrix= function(x, kernel.fn, ...) {
    # dim(X) rows and dim(Y) cols matrix
    outer(X= x, Y= x, FUN= function(a, b) {kernel.fn(a, b, ...)})
}

cov.matrix2= function(x, y, kernel.fn, ...) {
    # dim(X) rows and dim(Y) cols matrix
    outer(X= x, Y= y, FUN= function(a, b) {kernel.fn(a, b, ...)})
}

# given x coordinates, take N draws from kernel function at those points
drawSamples= function(x, N, seed, kernel.fn, ...) {
    Y= matrix(NA, nrow= length(x), ncol = N)
    
    set.seed(seed)
    for (n in 1: N) {
        K= cov.matrix(x, kernel.fn, ...)
        Y[, n]= mvrnorm(n= 1, mu = rep(0, times= length(x)), Sigma = K)
    }
    return(Y)
}

# x and y are 2 vectors
se.kernel= function(x, y, sigma.f, l) {
    return(sigma.f^2 * exp(-(x- y)^2 / (2* l^2)))
}

se.kernel2= function(x, y, l) {
    return(exp(-(x- y)^2 / (2* l^2)))
}

sigma= 0.1
sigmaf= 1
l= 1

x0= seq(-5.5, 5.5, length.out = 1000)  

# what does the prior look like?
Y1= drawSamples(x= x0, N= 5, kernel.fn = se.kernel, 
                seed= 21287, sigma.f= sigmaf, l= l)
col_list= c("blue", "purple", "orange", "chocolate", "green")

plot(range(x0), range(Y1), xlab = "x", ylab = "y", type = "n",
     main = expression(paste("SE kernel,", sigma[f],"=1, l=1")))

# draw 5 random samples from prior f(x)~ N(0, K), K= sigma_f^2* K0
# where K is squared expon. kernel w/ parameters sigma_f and l

for (n in 1:5) {
    lines(x0, Y1[, n], col = col_list[n], lwd = 1.5)
}

# suppose we observed 15 data points of (x, y)
set.seed(21287)
x= x0[sample(c(1:length(x0)), 15)]

n= length(x)
K0= cov.matrix(x= x, kernel.fn= se.kernel2, l= l)
K= sigmaf^2* K0
set.seed(21207)
y= mvrnorm(n= 1, mu = rep(0, times= n), Sigma = K)
plot(x= x, y= y)

# now we have a new input x.star
x.star= seq(from= -5.5, to= 5.5, length.out = 201)

# denote f(x.star)=f(x*)=f*
# draw 100 samples from post. dist of f*|x*, x, f (page 16)

gpCondCalc= function(x, x.star, y, sigma, sigmaf, l) {
    
    K.xstar.x= cov.matrix2(x= x.star, y= x, 
                           kernel.fn= se.kernel, sigma.f= sigmaf, l= l)
    
    K.x.x= cov.matrix2(x= x, y= x, 
                       kernel.fn= se.kernel, sigma.f= sigmaf, l= l)
    
    K.x.xstar= cov.matrix2(x= x, y= x.star, 
                           kernel.fn= se.kernel, sigma.f= sigmaf, l= l)
    
    K.xstar.xstar= cov.matrix2(x= x.star, y= x.star, 
                               kernel.fn= se.kernel, sigma.f= sigmaf, l= l)
    
    I= diag(length(x))
    
    mean.f.star= K.xstar.x %*% solve(K.x.x+ sigma^2* diag(n)) %*% y
    cov.f.star= K.xstar.xstar- K.xstar.x %*% solve(K.x.x+ sigma^2* diag(n)) %*% K.x.xstar
    
    cov.f.star.pred= cov.f.star+ sigma^2* diag(length(x.star))
    
    return(list(mean.f.star= mean.f.star, cov.f.star= cov.f.star,
                cov.f.star.pred= cov.f.star.pred))
}

# for a given dataset (x_i, y_i), i=1,2,...,n, and a given grid of test input/points,
# the following quantities are fixed, and thus we can compute in advance and use them directly in generating MVN samples

# here condition on f since y= f+ 0= f (b/c sigma= 0), which is the noise-free obs.
# The intuition behind this step is that the training points constrain the set of functions to those that pass through the training points.

out= gpCondCalc(x= x, x.star= x.star, y= y, sigma= 0, sigmaf= 1, l= 1)
mean.f.star= out$mean.f.star
cov.f.star= out$cov.f.star
cov.f.star.pred= out$cov.f.star.pred

M= 100
EB.mat= matrix(NA, nrow= length(x.star), ncol= M)
EB.pred.mat= matrix(NA, nrow= length(x.star), ncol= M)

set.seed(21295)
for(s in 1: M) {
    EB.mat[, s]= mvrnorm(n= 1, mu = mean.f.star, Sigma = cov.f.star)
    
    EB.pred.mat[, s]= mvrnorm(n= 1, mu = mean.f.star, Sigma = cov.f.star.pred)
}

postMean2= rowMeans(EB.mat)

postCredInt2= apply(EB.mat, MARGIN= 1, FUN = quantile, probs= c(0.025, 0.975))
postCredInt2= t(postCredInt2)

predCredInt2= apply(EB.pred.mat, MARGIN= 1, FUN = quantile, probs= c(0.025, 0.975))
predCredInt2= t(predCredInt2)

EBPlot.df= data.frame(x.star= x.star, mean= postMean2, 
                      LL= postCredInt2[,1], UL= postCredInt2[,2],
                      pLL= predCredInt2[,1], pUL= predCredInt2[,2])
dat2= data.frame(x= x, y= y)


# we can see post. sample path (draws) go through observed data points
plot(x= x, y= y, pch= 20, col=rgb(0.3,0.7,0.4,0.7), cex= 1.5,
     main= "", ylim= c(-3,3))
lines(x= x.star, y= EB.mat[,10], col= "pink")
lines(x= x.star, y= EB.mat[,11], col= "red")
lines(x= x.star, y= EB.mat[,12], col= "blue")



# plot data points, post. mean and 95% credible intervals 
plot(x= x, y= y, pch= 20, col=rgb(0.3,0.7,0.4,0.3), cex= 1.5,
     ylim= c(-2,2), main= "n=15")
lines(x= x.star, y= EBPlot.df$mean, col= "red")
lines(x= x.star, y= EBPlot.df$LL, col= "pink")
lines(x= x.star, y= EBPlot.df$UL, col= "pink")
#lines(x= x.star, y= EBPlot.df$pLL, col= "pink")
#lines(x= x.star, y= EBPlot.df$pUL, col= "pink")
legend("bottomleft", legend= c("post. mean f", "95% cred. interval"), 
       col= c("red", "pink"),
       lty= c(1,1), bty= "n")


library("reshape2")
library("ggplot2")

# after observing data, the uncertainty of the prediction using x* (x.star) is small in regions 
# close to the training data and grows as we move further away from those points.
# If a predicted point lies on the training data, there is no correlation with other points. Therefore, the function must pass directly through it.

melted_cormat.0= melt(K.xstar.xstar)
ggplot(data = melted_cormat.0, aes(x=Var1, y=Var2, fill=value)) + 
    geom_tile()
melted_cormat.1= melt(cov.f.star)
ggplot(data = melted_cormat.1, aes(x=Var1, y=Var2, fill=value)) + 
    geom_tile()

