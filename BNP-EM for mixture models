# https://stephens999.github.io/fiveMinuteStats/intro_to_em.html
# https://www.r-bloggers.com/sampling-paths-from-a-gaussian-process/
# https://distill.pub/2019/visual-exploration-gaussian-processes/
# https://www.r-bloggers.com/gaussian-process-regression-with-r/
# https://www.r-bloggers.com/a-glimpse-on-gaussian-process-regression/
# http://juliejosse.com/wp-content/uploads/2018/07/LectureNotesMissing.html#2)_ml_inference_with_missing_values
# http://www.cse.psu.edu/~rtc12/CSE586/lectures/EMLectureFeb3.pdf
# http://sia.webpopix.org/mixtureModels.html

# a func. to calc. logLik
compute.logLik= function(w, mu, sigma, y) {
    K= length(w)
    aa= sapply(c(1:K), function(x) 
    {w[x]* dnorm(x= y, mean= mu[x], sd= sigma[x]) } )
    return(sum(log(rowSums(aa))))
}


calc.gamma.zi.k= function(w, mu, sigma, y) {
    #n= length(y)
    K= length(w)
    #gamma.zi.k.mat= matrix(NA, ncol= K, nrow= n)
    
    gamma.zi.k.mat= sapply(c(1:K), function(k) {
        w[k]*dnorm(x= y, mean= mu[k], sd= sigma[k])
    } )
    gamma.zi.k.mat= gamma.zi.k.mat/ rowSums(gamma.zi.k.mat)
    
    return(gamma.zi.k.mat)
}

# E step, calc. expectation of complete data logLik
E.step= function(w, mu, sigma, y) {
    K= length(w)
    
    gamma.zi.k= calc.gamma.zi.k(w= w, mu= mu, sigma= sigma, y= y)
    
    aa= sapply(c(1:K), function(k) {
        gamma.zi.k[,k]* log(w[k])+ 
            gamma.zi.k[,k]*dnorm(x= y, mean= mu[k], sd= sigma[k], log= T) } )
    return(sum(rowSums(aa)))
}

# maximize para. for complete data logLik
M.step= function(w, mu, sigma, y) {
    K= length(w)
    n= length(y)
    
    gamma.zi.k= calc.gamma.zi.k(w= w, mu= mu, sigma= sigma, y= y)
    
    N.k= colSums(gamma.zi.k)
    
    mu= sapply(c(1:K), function(k) {
        sum(gamma.zi.k[,k]* y)/N.k[k] } )
    sigma.sq= sapply(c(1:K), function(k) {
        sum(gamma.zi.k[,k]* (y- mu[k])^2)/N.k[k] } )
    sigma= sqrt(sigma.sq)
    w= N.k/n
    return(list(mu= mu, sigma= sigma, w= w))
}

# specify initial values
init= list(w= c(0.2, 0.8), mu= c(1, 2), sigma= c(1/2, 1/3))

# init= initial values
# y= data points (a vector)
# epsilon= error margin
run.EM= function(init, y, epsilon= 1e-6) {
    # initial values
    w= init$w
    mu= init$mu
    sigma= init$sigma
    
    # vector of expected complete data logLik, for monitoring convergence
    exp.ll.vec= c()
    # vector to store logLik of observed data
    ll.vec= c()
    
    diff.exp.ll= 1
    count= 0
    
    # since logLik is likely to be neg., take absolute values!
    while(abs(diff.exp.ll)> epsilon) {
        gamma.zi.k= calc.gamma.zi.k(w= w, mu= mu, sigma= sigma, y= y)
        
        # E step
        exp.ll= E.step(w= w, mu= mu, sigma= sigma, y= y)
        
        # M step
        param= M.step(w= w, mu= mu, sigma= sigma, y= y)
        mu= param$mu; mu
        sigma= param$sigma; sigma
        w= param$w; w
        
        exp.ll.vec= c(exp.ll.vec, exp.ll)
        count= count+ 1
        
        if(count> 1) {
            diff.exp.ll= exp.ll.vec[count]- exp.ll.vec[count-1]
        }
        
        # calc. logLik of data (instead of expectation of complete data logLik)
        logLik= compute.logLik(w= w, mu= mu, sigma= sigma, y= y)
        ll.vec= c(ll.vec, logLik)
    }
    
    return(list(logLik.vec= ll.vec, mu= mu, sigma= sigma, 
                w= w, iter= count, logLik= ll.vec[count]))
    
}


# simulate some data
set.seed(21205)
y= true.F(n= 10000)

hist(y, breaks= 'Scott', freq= F)
grid= seq(from= min(y), to= max(y), length.out = 500)
lines(x= grid,y= 0.25*dnorm(grid,5, 1.5)+0.75*dnorm(grid,10,2),
      col= 2, lwd= 2)

out= run.EM(init= init, y= y, epsilon= 1e-7)
print(c(out$mu, out$sigma, out$w, out$logLik, out$iter))
# [1]     10.031132      4.999786      1.982725      1.460170      0.745716
# [6]      0.254284 -24330.981864    333.000000

# true logLik 
ll.truth= compute.logLik(w= c(1/4, 3/4), mu= c(5, 10), sigma= c(1.5, 2), y= y) # -24332.88
ll.truth

plot(x= c(1:out$iter), y= out$logLik.vec, pch= 20,
     xlab= "iterations", ylab= "logLik")
abline(h= ll.truth, lty= 2, col= 'red', lwd= 2)

# call a package
library("mixtools")
r.mixEM = normalmixEM(y)
list(p=r.mixEM$lambda,mu=r.mixEM$mu,sigma=r.mixEM$sigma)


